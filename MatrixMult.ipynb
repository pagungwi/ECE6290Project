{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0ac152-cc8d-44f0-b3d1-432003a475a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 531, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/local_scratch/slurm.6808297/ipykernel_1659140/2026389619.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/torch/__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/torch/functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/peyabi/.conda/envs/ece4420/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /croot/pytorch-select_1700158693612/work/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ad16ee-f647-48c2-b3de-d9c910959d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU explicitly\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Optional: tune number of threads to your machine\n",
    "torch.set_num_threads(max(1, os.cpu_count() or 1))\n",
    "\n",
    "\n",
    "def blocked_matmul(A: torch.Tensor, B: torch.Tensor, tile: int = 64) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Blocked (tiled) matrix multiplication for CPU using PyTorch tensors.\n",
    "    A: (N, K)\n",
    "    B: (K, M)\n",
    "    tile: tile size (tunable; typical 32..256)\n",
    "    Returns: C (N, M) computed as A @ B.\n",
    "\n",
    "    Implementation notes:\n",
    "    - Uses torch.mm on tile blocks to let CPU BLAS handle inner block multiply.\n",
    "    - Keeps control over tiling to improve cache locality.\n",
    "    - Works with float32/float64; ensure tensors are on CPU.\n",
    "    \"\"\"\n",
    "    assert A.device.type == \"cpu\" and B.device.type == \"cpu\", \"Only CPU tensors supported in this kernel\"\n",
    "    N, K = A.shape\n",
    "    K2, M = B.shape\n",
    "    assert K == K2, \"Inner dim mismatch\"\n",
    "\n",
    "    # Allocate result\n",
    "    C = torch.zeros((N, M), dtype=A.dtype, device=\"cpu\")\n",
    "\n",
    "    # Make contiguous for best performance\n",
    "    A = A.contiguous()\n",
    "    B = B.contiguous()\n",
    "\n",
    "    # Outer loops iterate tiles\n",
    "    for i in range(0, N, tile):\n",
    "        i_end = min(i + tile, N)\n",
    "        A_block = A[i:i_end]               # shape (ti, K)\n",
    "        for j in range(0, M, tile):\n",
    "            j_end = min(j + tile, M)\n",
    "            # We operate on this sub-block of C\n",
    "            C_block = C[i:i_end, j:j_end]\n",
    "            # Accumulate across K in tiles\n",
    "            for p in range(0, K, tile):\n",
    "                p_end = min(p + tile, K)\n",
    "                # Multiply small blocks: (ti,k_block) @ (k_block,tj) -> (ti,tj)\n",
    "                C_block += torch.mm(A_block[:, p:p_end], B[p:p_end, j:j_end])\n",
    "            # Store accumulator back\n",
    "            C[i:i_end, j:j_end] = C_block\n",
    "    return C\n",
    "\n",
    "\n",
    "# Simple nn.Module wrapper (useful for integrating into model code)\n",
    "class BlockedMatMulModule(nn.Module):\n",
    "    def __init__(self, tile: int = 64):\n",
    "        super().__init__()\n",
    "        self.tile = tile\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "        return blocked_matmul(A, B, self.tile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1105d7-f19a-4442-a747-a1b1b86e3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAPL ENERGY READING FUNCTIONS\n",
    "def read_energy_uj(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return int(f.read().strip())\n",
    "\n",
    "def find_rapl_sensors():\n",
    "    \"\"\"Return list of RAPL energy sensors (energy_uj files).\"\"\"\n",
    "    base = \"/sys/class/powercap/intel-rapl\"\n",
    "    sensors = []\n",
    "    if not os.path.exists(base):\n",
    "        print(\"RAPL not available on this system.\")\n",
    "        return sensors\n",
    "\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        for name in files:\n",
    "            if name == \"energy_uj\":\n",
    "                sensors.append(os.path.join(root, name))\n",
    "    return sensors\n",
    "\n",
    "RAPL_SENSORS = find_rapl_sensors()\n",
    "\n",
    "def measure_energy_j(func, *args, **kwargs):\n",
    "    \"\"\"Run func and measure elapsed time + energy across all RAPL sensors.\"\"\"\n",
    "    if not RAPL_SENSORS:\n",
    "        raise RuntimeError(\"No RAPL sensors found!\")\n",
    "\n",
    "    # read initial energy\n",
    "    e_before = [read_energy_uj(s) for s in RAPL_SENSORS]\n",
    "\n",
    "    # run function\n",
    "    t0 = time.perf_counter()\n",
    "    out = func(*args, **kwargs)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # read final energy\n",
    "    e_after = [read_energy_uj(s) for s in RAPL_SENSORS]\n",
    "\n",
    "    # convert to Joules\n",
    "    e_joules = []\n",
    "    for before, after, sensor in zip(e_before, e_after, RAPL_SENSORS):\n",
    "        # handle wrap-around\n",
    "        if after < before:\n",
    "            diff = (after + (1 << 32)) - before\n",
    "        else:\n",
    "            diff = after - before\n",
    "        e_joules.append(diff / 1e6)  # µJ → J\n",
    "\n",
    "    exec_time = t1 - t0\n",
    "    return out, exec_time, e_joules, RAPL_SENSORS\n",
    "\n",
    "\n",
    "# FLOP COUNT \n",
    "# Energy efficiency is given by GFLOPs per Joule\n",
    "def matmul_flops(N, K, M):\n",
    "    # GEMM does 2*N*K*M FLOPs\n",
    "    return 2 * N * K * M\n",
    "\n",
    "\n",
    "# BENCHMARK DRIVER\n",
    "def bench_kernel(name, fn, A, B, N, K, M):\n",
    "    out, t, sensor_energy, sensors = measure_energy_j(fn, A, B)\n",
    "\n",
    "    total_energy = sum(sensor_energy)  # J\n",
    "    avg_power = total_energy / t       # W\n",
    "    flops = matmul_flops(N, K, M)\n",
    "    gflops = flops / t / 1e9\n",
    "    eff = flops / total_energy / 1e9   # GFLOP per Joule\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Time: {t*1000:.2f} ms\")\n",
    "    print(f\"Total Energy: {total_energy:.4f} J\")\n",
    "    print(f\"Average Power: {avg_power:.2f} W\")\n",
    "    print(f\"Throughput: {gflops:.2f} GFLOP/s\")\n",
    "    print(f\"Energy Efficiency: {eff:.4f} GFLOP/J\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4a4e56-a9a8-43f0-b56a-69d8476af81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== torch.matmul ===\n",
      "Time: 0.45 ms\n",
      "Total Energy: 0.1239 J\n",
      "Average Power: 273.92 W\n",
      "Throughput: 74.17 GFLOP/s\n",
      "Energy Efficiency: 0.2708 GFLOP/J\n",
      "\n",
      "=== blocked_matmul tile=64 ===\n",
      "Time: 2.09 ms\n",
      "Total Energy: 0.2297 J\n",
      "Average Power: 109.73 W\n",
      "Throughput: 16.03 GFLOP/s\n",
      "Energy Efficiency: 0.1461 GFLOP/J\n",
      "Max abs error vs BLAS: 4.1961669921875e-05\n",
      "\n",
      "=== naive kernel ===\n",
      "Time: 19.71 ms\n",
      "Total Energy: 2.3842 J\n",
      "Average Power: 120.97 W\n",
      "Throughput: 1.70 GFLOP/s\n",
      "Energy Efficiency: 0.0141 GFLOP/J\n",
      "Max abs error vs BLAS: 1.52587890625e-05\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(8)\n",
    "\n",
    "N, K, M = 256, 256, 256\n",
    "A = torch.randn((N, K), dtype=torch.float32)\n",
    "B = torch.randn((K, M), dtype=torch.float32)\n",
    "\n",
    "# Baseline\n",
    "C_ref = bench_kernel(\"torch.matmul\", torch.matmul, A, B, N, K, M)\n",
    "\n",
    "# Blocked\n",
    "tile = 64\n",
    "C_block = bench_kernel(f\"blocked_matmul tile={tile}\",\n",
    "                       lambda X, Y: blocked_matmul(X, Y, tile),\n",
    "                       A, B, N, K, M)\n",
    "print(\"Max abs error vs BLAS:\", (C_block - C_ref).abs().max().item())\n",
    "\n",
    "# Naive\n",
    "def naive(A, B):\n",
    "    N, K = A.shape\n",
    "    _, M = B.shape\n",
    "    C = torch.zeros((N, M), dtype=A.dtype)\n",
    "    for k in range(K):\n",
    "        C += A[:, k:k+1].matmul(B[k:k+1, :])\n",
    "    return C\n",
    "\n",
    "C_naive = bench_kernel(\"naive kernel\", naive, A, B, N, K, M)\n",
    "print(\"Max abs error vs BLAS:\", (C_naive - C_ref).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8f51d-0918-44ab-9a18-14934c15465c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7dd5f-903d-4e10-9ace-9d27113b1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU Version\n",
    "\n",
    "def gpu_blocked_matmul(A: torch.Tensor, B: torch.Tensor, tile: int = 128) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Blocked GPU matmul.\n",
    "    A: (N, K), B: (K, M)\n",
    "    tile: tile size for blocking (128 or 256 works well on GPUs)\n",
    "    \"\"\"\n",
    "    assert A.is_cuda and B.is_cuda, \"Tensors must be on GPU\"\n",
    "\n",
    "    N, K = A.shape\n",
    "    K2, M = B.shape\n",
    "    assert K == K2\n",
    "\n",
    "    C = torch.zeros((N, M), dtype=A.dtype, device=A.device)\n",
    "\n",
    "    A = A.contiguous()\n",
    "    B = B.contiguous()\n",
    "\n",
    "    for i in range(0, N, tile):\n",
    "        i_end = min(i + tile, N)\n",
    "        A_block = A[i:i_end]\n",
    "\n",
    "        for j in range(0, M, tile):\n",
    "            j_end = min(j + tile, M)\n",
    "            C_block = C[i:i_end, j:j_end]\n",
    "\n",
    "            for p in range(0, K, tile):\n",
    "                p_end = min(p + tile, K)\n",
    "                # GPU performs this multiply\n",
    "                C_block += torch.mm(A_block[:, p:p_end], B[p:p_end, j:j_end])\n",
    "\n",
    "            C[i:i_end, j:j_end] = C_block\n",
    "\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c9855-542c-4009-9359-6ca3005c980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUBlockedMatMulModule(nn.Module):\n",
    "    def __init__(self, tile=128):\n",
    "        super().__init__()\n",
    "        self.tile = tile\n",
    "\n",
    "    def forward(self, A: torch.Tensor, B: torch.Tensor):\n",
    "        return gpu_blocked_matmul(A, B, self.tile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6fdfd-81ba-4412-94d4-e4163eec9ce1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# move to GPU\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# baseline\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ece4420/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# # move to GPU\n",
    "# A = torch.randn(1024, 1024, device='cuda')\n",
    "# B = torch.randn(1024, 1024, device='cuda')\n",
    "\n",
    "# # baseline\n",
    "# torch.cuda.synchronize()\n",
    "# t0 = time.perf_counter()\n",
    "# C_ref = torch.matmul(A, B)\n",
    "# torch.cuda.synchronize()\n",
    "# t1 = time.perf_counter()\n",
    "# print(\"torch.matmul:\", (t1 - t0)*1000, \"ms\")\n",
    "\n",
    "# # blocked\n",
    "# torch.cuda.synchronize()\n",
    "# t0 = time.perf_counter()\n",
    "# C_blk = gpu_blocked_matmul(A, B, tile=128)\n",
    "# torch.cuda.synchronize()\n",
    "# t1 = time.perf_counter()\n",
    "# print(\"blocked:\", (t1 - t0)*1000, \"ms\")\n",
    "\n",
    "# # correctness\n",
    "# print(\"max error:\", (C_ref - C_blk).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1927c-a2c8-48bf-9e7e-faa561679644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"compiled with cuda:\", torch.version.cuda)\n",
    "print(\"device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9edb2e2-e61e-4964-aea0-833ad3bfffea",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2505587979.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    module load cuda/12.1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676837e-759d-4e1d-ab25-5f6d5c35642b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
