{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "152304a8-9b3a-4359-998b-3ade799d0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d502c07-fa1a-4f17-b957-1e8e8429ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot Product Attention (baseline)\n",
    "\n",
    "def sdpa_baseline(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q: (B, H, S, D)\n",
    "    K: (B, H, S, D)\n",
    "    V: (B, H, S, D)\n",
    "    \"\"\"\n",
    "    B, H, S, D = Q.shape\n",
    "\n",
    "    # 1) scores = Q @ K^T\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(D)  # (B, H, S, S)\n",
    "\n",
    "    # 2) softmax\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "    # 3) out = attn @ V\n",
    "    out = torch.matmul(attn, V)  # (B, H, S, D)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50fc585c-9448-44ce-85e2-c427e4ad66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAPL ENERGY READING FUNCTIONS\n",
    "def read_energy_uj(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return int(f.read().strip())\n",
    "\n",
    "def find_rapl_sensors():\n",
    "    \"\"\"Return list of RAPL energy sensors (energy_uj files).\"\"\"\n",
    "    base = \"/sys/class/powercap/intel-rapl\"\n",
    "    sensors = []\n",
    "    if not os.path.exists(base):\n",
    "        print(\"RAPL not available on this system.\")\n",
    "        return sensors\n",
    "\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        for name in files:\n",
    "            if name == \"energy_uj\":\n",
    "                sensors.append(os.path.join(root, name))\n",
    "    return sensors\n",
    "\n",
    "RAPL_SENSORS = find_rapl_sensors()\n",
    "\n",
    "def measure_energy_j(func, *args, **kwargs):\n",
    "    \"\"\"Run func and measure elapsed time + energy across all RAPL sensors.\"\"\"\n",
    "    if not RAPL_SENSORS:\n",
    "        raise RuntimeError(\"No RAPL sensors found!\")\n",
    "\n",
    "    # read initial energy\n",
    "    e_before = [read_energy_uj(s) for s in RAPL_SENSORS]\n",
    "\n",
    "    # run function\n",
    "    t0 = time.perf_counter()\n",
    "    out = func(*args, **kwargs)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # read final energy\n",
    "    e_after = [read_energy_uj(s) for s in RAPL_SENSORS]\n",
    "\n",
    "    # convert to Joules\n",
    "    e_joules = []\n",
    "    for before, after, sensor in zip(e_before, e_after, RAPL_SENSORS):\n",
    "        # handle wrap-around\n",
    "        if after < before:\n",
    "            diff = (after + (1 << 32)) - before\n",
    "        else:\n",
    "            diff = after - before\n",
    "        e_joules.append(diff / 1e6)  # µJ → J\n",
    "\n",
    "    exec_time = t1 - t0\n",
    "    return out, exec_time, e_joules, RAPL_SENSORS\n",
    "\n",
    "# FLOPs for attention\n",
    "def attention_flops(B, H, S, D):\n",
    "    \"\"\"\n",
    "    FLOPs breakdown:\n",
    "\n",
    "    1) QK^T: B * H * S * D * S * 2\n",
    "    2) softmax: negligible for FLOPs counting (you can include if needed)\n",
    "    3) (softmax) @ V: B * H * S * S * D * 2\n",
    "\n",
    "    We return total FLOPs.\n",
    "    \"\"\"\n",
    "    qk_flops = 2 * B * H * S * D * S\n",
    "    av_flops = 2 * B * H * S * S * D\n",
    "    return qk_flops + av_flops\n",
    "\n",
    "\n",
    "\n",
    "def bench_attention(name, fn, Q, K, V, B, H, S, D):\n",
    "    out, t, energies, sensors = measure_energy_j(fn, Q, K, V)\n",
    "\n",
    "    total_energy = sum(energies)\n",
    "    avg_power = total_energy / t\n",
    "    flops = attention_flops(B, H, S, D)\n",
    "\n",
    "    gflops_s = flops / t / 1e9\n",
    "    eff = flops / total_energy / 1e9\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Time: {t*1000:.2f} ms\")\n",
    "    print(f\"Total Energy: {total_energy:.4f} J\")\n",
    "    print(f\"Average Power: {avg_power:.2f} W\")\n",
    "    print(f\"Throughput: {gflops_s:.2f} GFLOP/s\")\n",
    "    print(f\"Energy Efficiency: {eff:.4f} GFLOP/J\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fd27f5e-330f-439e-989a-d4a692ff4413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== scaled dot-product attention ===\n",
      "Time: 14.72 ms\n",
      "Total Energy: 1.5965 J\n",
      "Average Power: 108.45 W\n",
      "Throughput: 1.14 GFLOP/s\n",
      "Energy Efficiency: 0.0105 GFLOP/J\n"
     ]
    }
   ],
   "source": [
    "# Run Attention Benchmark\n",
    "B, H, S, D = 1, 1, 256, 64\n",
    "\n",
    "Q = torch.randn((B, H, S, D), dtype=torch.float32)\n",
    "K = torch.randn((B, H, S, D), dtype=torch.float32)\n",
    "V = torch.randn((B, H, S, D), dtype=torch.float32)\n",
    "\n",
    "# Baseline attention\n",
    "out_attn = bench_attention(\"scaled dot-product attention\", sdpa_baseline,\n",
    "                           Q, K, V, B, H, S, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780ccb5-e7de-4ab6-bc81-c15910676e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
