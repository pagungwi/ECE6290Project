import torch


N = 1025 # token length

d = 64 # d_model

Br = 4
Bc = 16


Q_mat = torch.rand((N, d))
K_mat = torch.rand((N, d))
V_mat = torch.rand((N, d))

#print("Q_mat", Q_mat)
#print("K_mat", K_mat)
#print("V_mat", V_mat)


# Q_mat @ K_mat.T: shape = [N,N]
expected_softmax = torch.softmax(Q_mat @ K_mat.T, dim=1)

# shape = [N,d]
expected_attention = expected_softmax @ V_mat




#---------------------------------------------------------------------

N1 = 1024
N2 = 1

Q_rem = torch.zeros((4,  d))
K_rem = torch.zeros((16, d))
V_rem = torch.zeros((16, d))

Q_rem[0, :] = Q_mat[N1, :]
K_rem[0, :] = K_mat[N1, :]
V_rem[0, :] = V_mat[N1, :]



assert torch.allclose(Q_rem[0,:], Q_mat[N1,:])
assert torch.allclose(K_rem[0,:], K_mat[N1,:])
assert torch.allclose(V_rem[0,:], V_mat[N1,:])

#print("Q_rem", Q_rem)
#print("K_rem", K_rem)
#print("V_rem", V_rem)
#exit(0)



O = torch.zeros((N, d))



# Q: [N1=1024, 64]
for block_start_Br in range(0, N1, Br):
    block_end_Br = block_start_Br + Br

    Qi = Q_mat[block_start_Br:block_end_Br, :]  # shape Br x d

    Oi = torch.zeros((Br, d))  # shape Br x d
    li = torch.zeros((Br, 1))  # shape Br x 1
    mi = torch.full((Br, 1), -torch.inf)  # shape Br x 1


    # K,V: [N1=1024, 64]
    for block_start_Bc in range(0, N1, Bc):
        block_end_Bc = block_start_Bc + Bc

        Kj = K_mat[block_start_Bc:block_end_Bc, :]  # shape Bc x d
        Vj = V_mat[block_start_Bc:block_end_Bc, :]  # shape Bc x d

        Sij = Qi @ Kj.T


        #---------------------------------------------------
        # have a scaling factor: torch.exp(mij_hat - mi_new)
        #---------------------------------------------------
        '''
        mij_hat = torch.max(Sij, dim=1).values[:, None]

        pij_hat = torch.exp(Sij - mij_hat)

        lij_hat = torch.sum(pij_hat, dim=1)[:, None]

        mi_new = torch.max(torch.column_stack([mi, mij_hat]), dim=1).values[:, None]

        # only keep just one li
        #li_new = torch.exp(mi - mi_new) * li + torch.exp(mij_hat - mi_new) * lij_hat
        li = torch.exp(mi - mi_new) * li + torch.exp(mij_hat - mi_new) * lij_hat

        # no need to multiply li
        Oi = (torch.exp(mi - mi_new) * Oi) + (torch.exp(mij_hat - mi_new) * pij_hat) @ Vj

        mi = mi_new
        '''

        #---------------------------------------------------
        # no scaling factor: torch.exp(mij_hat - mi_new)
        #---------------------------------------------------
        #'''
        mij_hat = torch.max(Sij, dim=1).values[:, None]

        mi_new = torch.max(torch.column_stack([mi, mij_hat]), dim=1).values[:, None]

        pij_hat = torch.exp(Sij - mi_new)
        lij_hat = torch.sum(pij_hat, dim=1)[:, None]

        li = torch.exp(mi - mi_new) * li + lij_hat

        Oi = Oi * torch.exp(mi - mi_new) + pij_hat @ Vj

        mi = mi_new
        #'''
        #---------------------------------------------------

    print("-------------------------------------------------")

    #---------------------------------------------------
    # remaining K,V
    #---------------------------------------------------
    # K,V: [N2=1, 64]

    Sij = Qi @ K_rem.T # [4,16]

    #mij_hat = torch.max(Sij, dim=1).values[:, None]
    mij_hat = Sij[:, 0][:, None]
    print("mij_hat.shape", mij_hat.shape) # [4,1]


    mi_new = torch.max(torch.column_stack([mi, mij_hat]), dim=1).values[:, None]
    print("mi_new.shape", mi_new.shape) # [4,1]

    pij_hat = Sij.clone()
    print("pij_hat.shape", pij_hat.shape)
    print("Sij[:,0].shape", Sij[:,0].shape)
    print("torch.exp(Sij[:,0][:, None] - mi_new).shape",
          torch.exp(Sij[:,0][:, None] - mi_new).shape)
    #exit(0)

    print("pij_hat[:,0].shape", pij_hat[:,0].shape)

    pij_hat[:,0] = torch.exp(Sij[:,0][:, None] - mi_new).view(4*1)

    lij_hat = pij_hat[:,0][:, None]

    li = torch.exp(mi - mi_new) * li + lij_hat

    Oi = Oi * torch.exp(mi - mi_new) + pij_hat @ V_rem

    #---------------------------------------------------


    Oi = Oi / li
    O[block_start_Br:block_end_Br, :] = Oi



assert torch.allclose(O[:1024, :], expected_attention[:1024, :])


print("expected_attention: \n")
print(expected_attention)
print("\n\n")


print("O: \n")
print(O)
print("\n\n")
